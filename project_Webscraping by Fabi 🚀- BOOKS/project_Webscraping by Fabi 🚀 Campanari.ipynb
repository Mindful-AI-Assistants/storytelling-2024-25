{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tarefa WebScrapping - Instructions\n",
        "# Preencha as c√©lulas do caderno com o que se pede."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGp1CfMC5HgZ"
      },
      "source": [
        "## Tutorial Expandido de Web Scraping com BeautifulSoup\n",
        "*Professor Dr. Rooney R. A. Coelho*\n",
        "\n",
        "*Aluna Fabiana üöÄ Campanari*\n",
        "\n",
        "### Introdu√ß√£o\n",
        "Neste tutorial, vamos aprofundar nossas habilidades em web scraping, utilizando as bibliotecas Python `requests`, `BeautifulSoup` e `pandas` para extrair dados de um site de livros e armazen√°-los em um DataFrame do Pandas para an√°lise posterior.\n",
        "\n",
        "### C√≥digo Completo com Explica√ß√µes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuLhybql5jp8"
      },
      "source": [
        "### Pr√©-requisito\n",
        "\n",
        "Antes de come√ßar, instale as bibliotecas necess√°rias. Se ainda n√£o tiver, execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "wyB6oPv15pjU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /Users/fabicampanari/myenv/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /Users/fabicampanari/myenv/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /Users/fabicampanari/myenv/lib/python3.12/site-packages (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHU_iPZV5s7j"
      },
      "source": [
        "## 1. Importando Bibliotecas\n",
        "Primeiro, importe as bibliotecas necess√°rias. Execute esta c√©lula para que todas as fun√ß√µes estejam dispon√≠veis ao longo do c√≥digo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "RCmHabyk5rHs"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxEfb_ks5yDM"
      },
      "source": [
        "### 2. Definindo a URL e Fazendo a Requisi√ß√£o\n",
        "Aqui, definimos a URL do site e fazemos uma requisi√ß√£o HTTP para obter o conte√∫do HTML da p√°gina. Depois, utilizamos o BeautifulSoup para interpretar o HTML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "vvBgZrFi53KV"
      },
      "outputs": [],
      "source": [
        "# Definindo a URL do site\n",
        "url = 'http://books.toscrape.com/'\n",
        "\n",
        "# Fazendo a requisi√ß√£o HTTP e analisando o HTML\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eBwzL8954yt"
      },
      "source": [
        "> Explica√ß√£o: A fun√ß√£o requests.get() acessa o site e o BeautifulSoup ajuda a transformar o conte√∫do em uma estrutura f√°cil de navegar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivq87Gc157a9"
      },
      "source": [
        "# 3. Encontrando Todos os Livros na P√°gina\n",
        "\n",
        "Esta c√©lula extrai a lista de livros presentes na p√°gina, usando a estrutura HTML para encontrar os artigos que cont√™m as informa√ß√µes sobre os livros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "IK_457ly6HCI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:32<00:00,  1.54it/s]\n"
          ]
        }
      ],
      "source": [
        "books = []\n",
        "\n",
        "# Encontrando todos os livros na p√°gina\n",
        "for pag in tqdm.tqdm(range(1, 51)):\n",
        "    response = requests.get(f\"http://books.toscrape.com/catalogue/page-{pag}.html\")\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\").find_all(class_ = \"product_pod\")\n",
        "    for book in soup: \n",
        "        books.append(book)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak--VXDi6KGR"
      },
      "source": [
        "> Explica√ß√£o: Cada livro est√° contido em uma tag <article> com a classe product_pod. O m√©todo find_all() retorna uma lista de todos esses elementos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2owmbXvf6MpR"
      },
      "source": [
        "### 4. Extraindo Informa√ß√µes de Cada Livro\n",
        "\n",
        "Nesta etapa, vamos iterar sobre cada livro encontrado e extrair o t√≠tulo, o pre√ßo e a disponibilidade. Armazene essas informa√ß√µes em uma lista chamada data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "bUCUpaLj6SMy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 7410.21it/s]\n"
          ]
        }
      ],
      "source": [
        "# Extraindo informa√ß√µes de cada livro\n",
        "titles = []\n",
        "prices = []\n",
        "stocks = []\n",
        "images = []\n",
        "categories = []\n",
        "\n",
        "for book in tqdm.tqdm(books):\n",
        "    titles.append(book.find(\"h3\").find(\"a\")[\"title\"])\n",
        "    prices.append(float(book.find(\"div\", class_=\"product_price\").find(\"p\").get_text().replace(\"¬£\", \"\").replace(\"√Ç\", \"\")))\n",
        "    stocks.append(book.find(\"p\", class_=\"instock availability\").get_text(strip=True))\n",
        "    images.append(book.find(\"img\")[\"src\"])\n",
        "    categories.append(book.find(\"p\", class_=\"star-rating\")[\"class\"][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8IZ-cF6VXT"
      },
      "source": [
        "> Explica√ß√£o: Usamos .find() para localizar elementos espec√≠ficos dentro do book. O .text.strip() remove espa√ßos em branco extras. A lista data √© ent√£o preenchida com dicion√°rios contendo as informa√ß√µes de cada livro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgHrjfVz6aqL"
      },
      "source": [
        "### 5. Criando um DataFrame e Exibindo os Dados\n",
        "Agora que temos todos os dados em uma lista, vamos criar um DataFrame do Pandas para organizar e visualizar os dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando o DataFrame com todas as vari√°veis\n",
        "library = pd.DataFrame({\n",
        "    \"title\": titles,\n",
        "    \"price\": prices,\n",
        "    \"stock\": stocks,\n",
        "    \"image\": images,\n",
        "    \"category\": categories\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEcO1m4r6fPN"
      },
      "source": [
        "> Explica√ß√£o: O DataFrame df nos permite visualizar e manipular os dados de forma estruturada. A fun√ß√£o head() exibe as primeiras cinco linhas do DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkHAd_LS6kiV"
      },
      "source": [
        "Para finalizar, vamos salvar o DataFrame em um arquivo CSV chamado livros.csv, que conter√° todos os dados extra√≠dos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "8Il42k_r6Iu4"
      },
      "outputs": [],
      "source": [
        "library.to_csv(\"livros.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvando o DataFrame em um arquivo CSV na pasta de destino que voce esc olher\n",
        "library.to_csv(\"/Users/fabicampanari/Desktop/WebScrapping-BeautifulSoaup/livros.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nWGG5K36m3e"
      },
      "source": [
        "> Explica√ß√£o: A fun√ß√£o to_csv() do Pandas salva o DataFrame como um arquivo CSV. Definimos index=False para n√£o incluir o √≠ndice do DataFrame no arquivo final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxiSL9hf5Hgf"
      },
      "source": [
        "### Explica√ß√£o Detalhada\n",
        "1. **Importar bibliotecas:** Importamos as bibliotecas necess√°rias para fazer requisi√ß√µes HTTP (`requests`), analisar HTML (`BeautifulSoup`) e manipular dados (`pandas`).\n",
        "2. **Definir a URL:** Definimos a URL do site que queremos raspar.\n",
        "3. **Fazer a requisi√ß√£o:** Realizamos uma requisi√ß√£o HTTP para a URL e analisamos o conte√∫do HTML da p√°gina utilizando o BeautifulSoup.\n",
        "4. **Encontrar os livros:** Localizamos todos os elementos HTML que representam os livros na p√°gina, utilizando o m√©todo `find_all`.\n",
        "5. **Extrair informa√ß√µes:** Iteramos sobre cada elemento de livro e extra√≠mos o t√≠tulo, pre√ßo e disponibilidade, utilizando o m√©todo `find` para localizar elementos espec√≠ficos.\n",
        "6. **Criar DataFrame:** Criamos um DataFrame do Pandas para armazenar os dados extra√≠dos em uma estrutura tabular.\n",
        "7. **Exibir e salvar:** Exibimos as primeiras linhas do DataFrame para verificar os dados e salvamos o DataFrame em um arquivo CSV para an√°lise posterior.\n",
        "\n",
        "### Expans√µes e Personaliza√ß√µes\n",
        "* **Extrair mais informa√ß√µes:** Voc√™ pode extrair outras informa√ß√µes, como a imagem do livro, a categoria, o autor, etc., localizando os elementos HTML correspondentes.\n",
        "* **Limpar os dados:** Limpe os dados extra√≠dos, removendo caracteres especiais, formatando valores num√©ricos e tratando valores ausentes.\n",
        "* **Manipular o DataFrame:** Utilize as funcionalidades do Pandas para manipular o DataFrame, como filtrar, ordenar, agrupar e realizar c√°lculos.\n",
        "* **Visualizar os dados:** Crie gr√°ficos e visualiza√ß√µes para explorar os dados de forma mais intuitiva, utilizando bibliotecas como Matplotlib ou Seaborn.\n",
        "* **Pagina√ß√£o:** Se o site tiver v√°rias p√°ginas de resultados, implemente a pagina√ß√£o para extrair dados de todas as p√°ginas.\n",
        "* **Lidar com erros:** Utilize blocos `try-except` para lidar com poss√≠veis erros durante o processo de scraping, como p√°ginas n√£o encontradas ou elementos HTML ausentes.\n",
        "\n",
        "### Pr√≥ximos Passos\n",
        "* **Explorar outros sites:** Pratique com outros sites para aprimorar suas habilidades em web scraping.\n",
        "* **Aprender t√©cnicas avan√ßadas:** Explore t√©cnicas mais avan√ßadas, como XPath, CSS Selectors e bibliotecas como Scrapy.\n",
        "* **Construir projetos reais:** Utilize o web scraping para coletar dados para seus pr√≥prios projetos, como an√°lise de sentimentos, monitoramento de pre√ßos, etc.\n",
        "\n",
        "**Lembre-se:** √â importante respeitar os termos de servi√ßo dos sites que voc√™ est√° raspando e evitar sobrecarregar os servidores.\n",
        "\n",
        "Com este tutorial expandido, voc√™ possui uma base s√≥lida para iniciar seus projetos de web scraping e explorar o vasto mundo de dados dispon√≠veis na web."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
