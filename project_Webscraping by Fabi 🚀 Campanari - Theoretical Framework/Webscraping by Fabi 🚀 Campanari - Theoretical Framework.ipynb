{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instructions\n",
        "# Preencha as células do caderno com o que se pede."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGp1CfMC5HgZ"
      },
      "source": [
        "## Tutorial Expandido de Web Scraping com BeautifulSoup\n",
        "*Professor Dr. Rooney R. A. Coelho*\n",
        "\n",
        "\n",
        "### Introdução\n",
        "Neste tutorial, vamos aprofundar nossas habilidades em web scraping, utilizando as bibliotecas Python `requests`, `BeautifulSoup` e `pandas` para extrair dados de um site de livros e armazená-los em um DataFrame do Pandas para análise posterior.\n",
        "\n",
        "### Código Completo com Explicações"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuLhybql5jp8"
      },
      "source": [
        "### Pré-requisito\n",
        "\n",
        "Antes de começar, instale as bibliotecas necessárias. Se ainda não tiver, execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wyB6oPv15pjU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /Users/fabicampanari/myenv/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /Users/fabicampanari/myenv/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHU_iPZV5s7j"
      },
      "source": [
        "## 1. Importando Bibliotecas\n",
        "Primeiro, importe as bibliotecas necessárias. Execute esta célula para que todas as funções estejam disponíveis ao longo do código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RCmHabyk5rHs"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxEfb_ks5yDM"
      },
      "source": [
        "### 2. Definindo a URL e Fazendo a Requisição\n",
        "Aqui, definimos a URL do site e fazemos uma requisição HTTP para obter o conteúdo HTML da página. Depois, utilizamos o BeautifulSoup para interpretar o HTML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vvBgZrFi53KV"
      },
      "outputs": [],
      "source": [
        "# Definindo a URL do site\n",
        "url = 'http://books.toscrape.com/'\n",
        "\n",
        "# Fazendo a requisição HTTP e analisando o HTML\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eBwzL8954yt"
      },
      "source": [
        "> Explicação: A função requests.get() acessa o site e o BeautifulSoup ajuda a transformar o conteúdo em uma estrutura fácil de navegar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivq87Gc157a9"
      },
      "source": [
        "# 3. Encontrando Todos os Livros na Página\n",
        "\n",
        "Esta célula extrai a lista de livros presentes na página, usando a estrutura HTML para encontrar os artigos que contêm as informações sobre os livros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IK_457ly6HCI"
      },
      "outputs": [],
      "source": [
        "# Faça você!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak--VXDi6KGR"
      },
      "source": [
        "> Explicação: Cada livro está contido em uma tag <article> com a classe product_pod. O método find_all() retorna uma lista de todos esses elementos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2owmbXvf6MpR"
      },
      "source": [
        "### 4. Extraindo Informações de Cada Livro\n",
        "\n",
        "Nesta etapa, vamos iterar sobre cada livro encontrado e extrair o título, o preço e a disponibilidade. Armazene essas informações em uma lista chamada data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bUCUpaLj6SMy"
      },
      "outputs": [],
      "source": [
        "# Faça você!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8IZ-cF6VXT"
      },
      "source": [
        "> Explicação: Usamos .find() para localizar elementos específicos dentro do book. O .text.strip() remove espaços em branco extras. A lista data é então preenchida com dicionários contendo as informações de cada livro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgHrjfVz6aqL"
      },
      "source": [
        "### 5. Criando um DataFrame e Exibindo os Dados\n",
        "Agora que temos todos os dados em uma lista, vamos criar um DataFrame do Pandas para organizar e visualizar os dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nIuLNhOu6ddM"
      },
      "outputs": [],
      "source": [
        "# Faça você!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEcO1m4r6fPN"
      },
      "source": [
        "> Explicação: O DataFrame df nos permite visualizar e manipular os dados de forma estruturada. A função head() exibe as primeiras cinco linhas do DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkHAd_LS6kiV"
      },
      "source": [
        "Para finalizar, vamos salvar o DataFrame em um arquivo CSV chamado livros.csv, que conterá todos os dados extraídos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8Il42k_r6Iu4"
      },
      "outputs": [],
      "source": [
        "# Faça você!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nWGG5K36m3e"
      },
      "source": [
        "> Explicação: A função to_csv() do Pandas salva o DataFrame como um arquivo CSV. Definimos index=False para não incluir o índice do DataFrame no arquivo final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxiSL9hf5Hgf"
      },
      "source": [
        "### Explicação Detalhada\n",
        "1. **Importar bibliotecas:** Importamos as bibliotecas necessárias para fazer requisições HTTP (`requests`), analisar HTML (`BeautifulSoup`) e manipular dados (`pandas`).\n",
        "2. **Definir a URL:** Definimos a URL do site que queremos raspar.\n",
        "3. **Fazer a requisição:** Realizamos uma requisição HTTP para a URL e analisamos o conteúdo HTML da página utilizando o BeautifulSoup.\n",
        "4. **Encontrar os livros:** Localizamos todos os elementos HTML que representam os livros na página, utilizando o método `find_all`.\n",
        "5. **Extrair informações:** Iteramos sobre cada elemento de livro e extraímos o título, preço e disponibilidade, utilizando o método `find` para localizar elementos específicos.\n",
        "6. **Criar DataFrame:** Criamos um DataFrame do Pandas para armazenar os dados extraídos em uma estrutura tabular.\n",
        "7. **Exibir e salvar:** Exibimos as primeiras linhas do DataFrame para verificar os dados e salvamos o DataFrame em um arquivo CSV para análise posterior.\n",
        "\n",
        "### Expansões e Personalizações\n",
        "* **Extrair mais informações:** Você pode extrair outras informações, como a imagem do livro, a categoria, o autor, etc., localizando os elementos HTML correspondentes.\n",
        "* **Limpar os dados:** Limpe os dados extraídos, removendo caracteres especiais, formatando valores numéricos e tratando valores ausentes.\n",
        "* **Manipular o DataFrame:** Utilize as funcionalidades do Pandas para manipular o DataFrame, como filtrar, ordenar, agrupar e realizar cálculos.\n",
        "* **Visualizar os dados:** Crie gráficos e visualizações para explorar os dados de forma mais intuitiva, utilizando bibliotecas como Matplotlib ou Seaborn.\n",
        "* **Paginação:** Se o site tiver várias páginas de resultados, implemente a paginação para extrair dados de todas as páginas.\n",
        "* **Lidar com erros:** Utilize blocos `try-except` para lidar com possíveis erros durante o processo de scraping, como páginas não encontradas ou elementos HTML ausentes.\n",
        "\n",
        "### Próximos Passos\n",
        "* **Explorar outros sites:** Pratique com outros sites para aprimorar suas habilidades em web scraping.\n",
        "* **Aprender técnicas avançadas:** Explore técnicas mais avançadas, como XPath, CSS Selectors e bibliotecas como Scrapy.\n",
        "* **Construir projetos reais:** Utilize o web scraping para coletar dados para seus próprios projetos, como análise de sentimentos, monitoramento de preços, etc.\n",
        "\n",
        "**Lembre-se:** É importante respeitar os termos de serviço dos sites que você está raspando e evitar sobrecarregar os servidores.\n",
        "\n",
        "Com este tutorial expandido, você possui uma base sólida para iniciar seus projetos de web scraping e explorar o vasto mundo de dados disponíveis na web."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Expansões e Personalizações\n",
        "# Extrair mais informações: Você pode extrair outras informações, como a imagem do livro, a categoria, o autor, etc., localizando os elementos HTML correspondentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /Users/fabicampanari/myenv/lib/python3.12/site-packages (4.66.5)\n",
            "Requirement already satisfied: requests in /Users/fabicampanari/myenv/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /Users/fabicampanari/myenv/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/fabicampanari/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'books' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m categories \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m book \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[43mbooks\u001b[49m):\n\u001b[1;32m      6\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(book\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m     categories\u001b[38;5;241m.\u001b[39mappend(book\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstar-rating\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'books' is not defined"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "images = []\n",
        "categories = []\n",
        "for book in tqdm.tqdm(books):\n",
        "    images.append(book.find(\"img\")[\"src\"])\n",
        "    categories.append(book.find(\"p\", class_=\"star-rating\")[\"class\"][1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
